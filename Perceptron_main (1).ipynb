{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "FINAL PERCEPTRON MODEL"
      ],
      "metadata": {
        "id": "u6H-YLgFcuhw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Library Imports"
      ],
      "metadata": {
        "id": "B2skc3J_859e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*\tNumpy and Pandas are used for data manipulation and numerical operations.\n",
        "*\tScikit-learn provides tools for model creation, data splitting, scaling, and performance evaluation.\n",
        "*\tMLPClassifier, Perceptron, train_test_split, StandardScaler, and various metrics like accuracy_score and confusion_matrix are used for model building and evaluation.\n",
        "*\turllib is used to download the dataset from a URL.\n"
      ],
      "metadata": {
        "id": "pTUvAfae8rxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_svmlight_file\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import urllib.request"
      ],
      "metadata": {
        "id": "LenXrbs_aWAE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading and Conversion of Data"
      ],
      "metadata": {
        "id": "jFfIJyFp9Bd3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   The diabetes dataset in SVMLight format is downloaded using the URL and stored locally as 'diabetes_scale.libsvm'.\n",
        "*   load_svmlight_file loads the dataset from this file, returning the feature matrix (X) and target vector (y).\n",
        "*   X.toarray() converts the sparse matrix format of the dataset into a dense format (regular NumPy array).\n",
        "*   The target vector y, which originally has values of -1 and 1, is converted into binary labels 0 and 1 using the transformation (y + 1) // 2.\n"
      ],
      "metadata": {
        "id": "-lUc7T-l71LV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and prepare the data\n",
        "url = 'https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/diabetes_scale'\n",
        "urllib.request.urlretrieve(url, 'diabetes_scale.libsvm')\n",
        "X, y = load_svmlight_file('diabetes_scale.libsvm')\n",
        "\n",
        "# Convert the sparse matrix to dense format\n",
        "X_dense = X.toarray()\n",
        "\n",
        "# Convert target variable from -1 and 1 to 0 and 1\n",
        "y = (y + 1) // 2"
      ],
      "metadata": {
        "id": "c7i8Xwhx8CsT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Splitting"
      ],
      "metadata": {
        "id": "cbGeYbxX8JWV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*\ttrain_test_split divides the data into:\n",
        "  - Training set (70% of the data).\n",
        "  - Temporary set (30% of the data) that is split again into:\n",
        "        - Validation set (15% of the overall data).\n",
        "        - Test set (15% of the overall data).\n"
      ],
      "metadata": {
        "id": "df5IRtl59o1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training, validation, and test sets (70% training, 15% validation, 15% test)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_dense, y, test_size=0.3,random_state=1882025)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5,random_state=1882025)"
      ],
      "metadata": {
        "id": "8tXo6VM78I5S"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E0gCbWFxWhMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Scaling"
      ],
      "metadata": {
        "id": "bsoFQfwn-L_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**StandardScaler** is initialized to standardize the data (mean=0, variance=1). It is fitted to the training data and applied to both the training, validation, and test sets, ensuring consistent scaling across the data.\n"
      ],
      "metadata": {
        "id": "TsgwEQnS9mzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the training data and transform the datasets\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "L9fXKP4W9nK9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Customized Perceptron"
      ],
      "metadata": {
        "id": "UHZ8qik9-qQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part of the code defines a custom perceptron with different activation functions.\n",
        "\n",
        "- Attributes:\n",
        "  - **learning_rate**: The step size for adjusting weights.\n",
        "  -\t**n_iterations**: Number of training iterations.\n",
        "  -\t**activation**: The activation function, which could be 'step', 'sigmoid', or 'ReLU'.\n",
        "  -\t**with_bias**: Whether to use bias in the model.\n",
        "\n",
        "- Methods:\n",
        "  -\t**activate**: Defines the activation functions (step, sigmoid, or ReLU) applied to the model's linear output.\n",
        "  -\t**fit**: The method that trains the model by adjusting weights using the perceptron rule. For each data point, the weights are updated based on the difference between the predicted and actual values.\n",
        "  -\t**predict**: Uses the learned weights to make predictions on new data, applying the activation function to the weighted sum of inputs.\n"
      ],
      "metadata": {
        "id": "yfnYHhi5-wVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class defined_Perceptron:\n",
        "    def __init__(self, learning_rate=0.01, n_iterations=1000, activation='step', with_bias=True):\n",
        "        self.lr = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.activation = activation\n",
        "        self.with_bias = with_bias\n",
        "        self.weights = None\n",
        "        self.bias = 0 if with_bias else None\n",
        "\n",
        "    # Single function describing the different activation functions which can help us fit the model with different activation functions.\n",
        "    def activate(self, x):\n",
        "        if self.activation == 'step':\n",
        "            return 1 if x >= 0 else 0\n",
        "        elif self.activation == 'sigmoid':\n",
        "            return 1 / (1 + np.exp(-x))\n",
        "        elif self.activation == 'relu':\n",
        "            return max(0, x)\n",
        "\n",
        "    # Fitting the data with varied weights and bias (both with and without)\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "\n",
        "        for _ in range(self.n_iterations):\n",
        "            for idx, x_i in enumerate(X):\n",
        "                linear_output = np.dot(x_i, self.weights) + (self.bias if self.with_bias else 0)\n",
        "                y_predicted = self.activate(linear_output)\n",
        "\n",
        "                update = self.lr * (y[idx] - y_predicted)\n",
        "                self.weights += update * x_i\n",
        "                if self.with_bias:\n",
        "                    self.bias += update\n",
        "\n",
        "    # Function to predict the model\n",
        "    def predict(self, X):\n",
        "        linear_output = np.dot(X, self.weights) + (self.bias if self.with_bias else 0)\n",
        "        # Apply thresholding\n",
        "        # Coverting continous to binary\n",
        "        return np.array([1 if self.activate(x) >= 0.5 else 0 for x in linear_output])"
      ],
      "metadata": {
        "id": "N_LeVnJU-xf3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "njymh2fPAgzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following models are initialized:\n",
        "-\tBasic Perceptron: In-built perceptron from scikit-learn.\n",
        "-\tMLP (Multi-Layer Perceptron): In-built artificial neural network model from scikit-learn with one hidden layer.\n",
        "-\tCustom Perceptron Models: Customised perceptron with various configurations according to:\n",
        "  -\tWhether they use a bias.\n",
        "  -\tThe type of activation function (ReLU, sigmoid, or step).\n"
      ],
      "metadata": {
        "id": "sIQWFZTtAh_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models\n",
        "models = {\n",
        "    'Basic Perceptron': Perceptron(random_state=1882025),\n",
        "    'MLP': MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000),\n",
        "    'Perceptron No Bias': defined_Perceptron(with_bias=False),\n",
        "    'Perceptron Updated Bias': defined_Perceptron(with_bias=True),\n",
        "    'Perceptron ReLU': defined_Perceptron(activation='relu'),\n",
        "    'Perceptron Sigmoid': defined_Perceptron(activation='sigmoid')\n",
        "}"
      ],
      "metadata": {
        "id": "2cTm4bRFAiaW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Evaluation"
      ],
      "metadata": {
        "id": "Ari76H7FBG38"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each model:\n",
        "-\tTraining:\n",
        "  -\tScikit-learn models (Basic Perceptron and MLP) use their built-in fit function.\n",
        "  -\tThe custom perceptron models use the fit method defined in the CustomPerceptron class.\n",
        "-\tPrediction: The models predict labels for the test set (X_test_scaled).\n",
        "-\tEvaluation:\n",
        "  -\tAccuracy is computed using accuracy_score.\n",
        "  -\tThe confusion matrix is computed, which provides detailed performance information:\n",
        "      -\tTrue Positives, True Negatives, False Positives, and False Negatives.\n",
        "      -\tThe number of false negatives (cases where the model predicted 0 but the actual label was 1) is extracted from the confusion matrix to evaluate how often the model misses positive cases.\n"
      ],
      "metadata": {
        "id": "FOvZYgIYBK2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store the evaluation results\n",
        "results = {}\n",
        "\n",
        "# Train and evaluate models\n",
        "for name, model in models.items():\n",
        "    if isinstance(model, MLPClassifier) or isinstance(model, Perceptron):\n",
        "        # For scikit-learn models\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "    else:\n",
        "        # For custom Perceptron models\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "    # Calculate accuracy and confusion matrix\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    false_negatives = cm[1, 0]  # False negatives (assuming class 1 is positive)\n",
        "\n",
        "    # Store the results\n",
        "    results[name] = {\n",
        "        'Accuracy': accuracy,\n",
        "        'False Negatives': false_negatives,\n",
        "        'Confusion Matrix': cm\n",
        "    }"
      ],
      "metadata": {
        "id": "39GN9hh7BLye",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4d86ba77-6b58-4ec6-e8f6-18f42641434c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Results"
      ],
      "metadata": {
        "id": "zVRRlVf1BpxK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-\tThe accuracy and false negatives for each model are stored in a dictionary (results).\n",
        "-\tThe model with the highest accuracy is identified and printed as the \"Best Model.\"\n"
      ],
      "metadata": {
        "id": "lLwxhIBvBsjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_name = None\n",
        "best_accuracy = 0\n",
        "\n",
        "print(\"Model Comparison Results:\")\n",
        "for model, metrics in results.items():\n",
        "    print(f\"\\nModel: {model}\")\n",
        "    print(f\"Accuracy: {metrics['Accuracy']:.4f}\")\n",
        "    print(f\"False Negatives: {metrics['False Negatives']}\")\n",
        "    print(f\"Confusion Matrix:\\n{metrics['Confusion Matrix']}\")\n",
        "\n",
        "    # Track the best model\n",
        "    if metrics['Accuracy'] > best_accuracy:\n",
        "        best_accuracy = metrics['Accuracy']\n",
        "        best_model_name = model\n",
        "\n",
        "# Output the best model\n",
        "print(f\"\\nBest Model: {best_model_name}\")\n",
        "print(f\"Best Accuracy: {best_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "ejkb-M3vBtWX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "1096b71f-bb3e-4fe1-963c-3e02e3b82d68"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Comparison Results:\n",
            "\n",
            "Model: Basic Perceptron\n",
            "Accuracy: 0.6897\n",
            "False Negatives: 9\n",
            "Confusion Matrix:\n",
            "[[11 27]\n",
            " [ 9 69]]\n",
            "\n",
            "Model: MLP\n",
            "Accuracy: 0.7328\n",
            "False Negatives: 11\n",
            "Confusion Matrix:\n",
            "[[18 20]\n",
            " [11 67]]\n",
            "\n",
            "Model: Perceptron No Bias\n",
            "Accuracy: 0.5776\n",
            "False Negatives: 30\n",
            "Confusion Matrix:\n",
            "[[19 19]\n",
            " [30 48]]\n",
            "\n",
            "Model: Perceptron Updated Bias\n",
            "Accuracy: 0.6293\n",
            "False Negatives: 22\n",
            "Confusion Matrix:\n",
            "[[17 21]\n",
            " [22 56]]\n",
            "\n",
            "Model: Perceptron ReLU\n",
            "Accuracy: 0.7672\n",
            "False Negatives: 7\n",
            "Confusion Matrix:\n",
            "[[18 20]\n",
            " [ 7 71]]\n",
            "\n",
            "Model: Perceptron Sigmoid\n",
            "Accuracy: 0.7759\n",
            "False Negatives: 7\n",
            "Confusion Matrix:\n",
            "[[19 19]\n",
            " [ 7 71]]\n",
            "\n",
            "Best Model: Perceptron Sigmoid\n",
            "Best Accuracy: 0.7759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification Report"
      ],
      "metadata": {
        "id": "2DbhDzVwB425"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the best-performing model, a detailed classification report is printed using scikit-learn's classification_report. This includes precision, recall, F1-score, and support for each class.\n"
      ],
      "metadata": {
        "id": "xv6xc5JvB72E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report for the best model\n",
        "if best_model_name in models:\n",
        "    best_model = models[best_model_name]\n",
        "    y_pred_best = best_model.predict(X_test_scaled)\n",
        "\n",
        "    print(f\"\\nClassification Report for {best_model_name}:\\n\")\n",
        "    print(classification_report(y_test, y_pred_best))"
      ],
      "metadata": {
        "id": "febXJZWwB8QC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "73a45fc3-052b-4af3-b467-da48cd6be16e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report for Perceptron Sigmoid:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.72      0.58      0.64        45\n",
            "         1.0       0.76      0.86      0.81        71\n",
            "\n",
            "    accuracy                           0.75       116\n",
            "   macro avg       0.74      0.72      0.72       116\n",
            "weighted avg       0.75      0.75      0.74       116\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Key aspects of the custom perceptron class"
      ],
      "metadata": {
        "id": "yzyAJGrwDXjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Flexible Activation Functions makes the CustomPerceptron versatile. Each activation function brings unique benefits:\n",
        "\n",
        "  - The step function makes it model suitable for binary classification tasks.\n",
        "  - The sigmoid function enables smooth decision boundaries and probabilistic outputs.\n",
        "  - ReLU provides flexibility for more complex models, often used in deeper networks.\n",
        "\n",
        "  We checked how these conditions affect the performance of the model.\n",
        "\n",
        "- Bias Handling: The model can be configured with or without bias. Including bias gives the perceptron more flexibility, allowing it to better fit datasets where the decision boundary doesn't pass through the origin.\n",
        "\n",
        "- Gradient-Free Weight Update: The perceptron updates its weights using a simple rule based on prediction errors. This makes it computationally less intensive and easier to understand, though it limits the model to linearly separable problems.\n",
        "\n",
        "- Thresholding Mechanism: After applying the activation function, the perceptron outputs continuous values (for sigmoid or ReLU), but the final classification is binary. So a conversion is done to make it binary and then the threshold is applied. A threshold of 0.5 is applied in the customised perceptron"
      ],
      "metadata": {
        "id": "FtDvR2FfFaE1"
      }
    }
  ]
}